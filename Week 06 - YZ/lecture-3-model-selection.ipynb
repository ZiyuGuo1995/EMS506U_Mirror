{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c31a524f",
   "metadata": {},
   "source": [
    "# Feature Engineering and Model Selection\n",
    "\n",
    "The section on model selection, particularly section 1.2 is important for the coursework\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c57e03",
   "metadata": {},
   "source": [
    "## 1 Model selection\n",
    "\n",
    "A ML strategy might looko like:\n",
    "\n",
    "\n",
    "    Choose a class of model\n",
    "    Choose model hyperparameters\n",
    "    Fit the model to the training data\n",
    "    Use the model to predict labels for new data\n",
    "\n",
    "The first two pieces of this—the choice of model and choice of hyperparameters—are perhaps the most important part of using these tools and techniques effectively. In order to make an informed choice, we need a way to validate that our model and our hyperparameters are a good fit to the data. While this may sound simple, there are some pitfalls that you must avoid to do this effectively.\n",
    "\n",
    "### 1.1 Model validation the wrong way\n",
    "\n",
    "Let's demonstrate the naive approach to validation using the Iris data, which we saw in the previous section. We will start by loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7c4fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de4531e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614080e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e70499a",
   "metadata": {},
   "source": [
    "We see an accuracy score of 1.0, which indicates that 100% of points were correctly labeled by our model! But is this truly measuring the expected accuracy? Have we really come upon a model that we expect to be correct 100% of the time?\n",
    "\n",
    "As you may have gathered, the answer is no. In fact, this approach contains a fundamental flaw: it trains and evaluates the model on the same data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf2925e",
   "metadata": {},
   "source": [
    "### 1.2 Model validation the right way\n",
    "\n",
    "So what can be done? A better sense of a model's performance can be found using what's known as a holdout set: that is, we hold back some subset of the data from the training of the model, and then use this holdout set to check the model performance. This splitting can be done using the `train_test_split` utility in Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a637d10b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9963ae9",
   "metadata": {},
   "source": [
    "We see here a more reasonable result: the nearest-neighbor classifier is about 90% accurate on this hold-out set. The hold-out set is similar to unknown data, because the model has not \"seen\" it before.\n",
    "    \n",
    "### 1.3 Cross-validation\n",
    "\n",
    "One disadvantage of using a holdout set for model validation is that we have lost a portion of our data to the model training. In the above case, half the dataset does not contribute to the training of the model! This is not optimal, and can cause problems – especially if the initial set of training data is small.\n",
    "\n",
    "One way to address this is to use cross-validation; that is, to do a sequence of fits where each subset of the data is used both as a training set and as a validation set.\n",
    "\n",
    "Here we split the data into five groups, and use each of them in turn to evaluate the model fit on the other 4/5 of the data. This would be rather tedious to do by hand, and so we can use Scikit-Learn's cross_val_score convenience routine to do it succinctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611216dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b98dd884",
   "metadata": {},
   "source": [
    "\n",
    "## 2 Feature engineering\n",
    "\n",
    "In this section, we will cover a few common examples of feature engineering tasks: features for representing categorical data, features for representing text, and features for representing images. Additionally, we will discuss derived features for increasing model complexity and imputation of missing data.\n",
    "\n",
    "### Categorical data\n",
    "\n",
    "One common type of non-numerical data is categorical data. For example, imagine you are exploring some data on housing prices, and along with numerical features like \"price\" and \"rooms\", you also have \"neighborhood\" information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eac877",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {'price': 850000, 'rooms': 4, 'neighborhood': 'Queen Anne'},\n",
    "    {'price': 700000, 'rooms': 3, 'neighborhood': 'Fremont'},\n",
    "    {'price': 650000, 'rooms': 3, 'neighborhood': 'Wallingford'},\n",
    "    {'price': 600000, 'rooms': 2, 'neighborhood': 'Fremont'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a835d8a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fdf2ba9",
   "metadata": {},
   "source": [
    "It turns out that this is not generally a useful approach in Scikit-Learn: the package's models make the fundamental assumption that numerical features reflect algebraic quantities. Thus such a mapping would imply, for example, that Queen Anne < Fremont < Wallingford, or even that Wallingford - Queen Anne = Fremont, which (niche demographic jokes aside) does not make much sense.\n",
    "\n",
    "In this case, one proven technique is to use one-hot encoding, which effectively creates extra columns indicating the presence or absence of a category with a value of 1 or 0, respectively. When your data comes as a list of dictionaries, Scikit-Learn's DictVectorizer will do this for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a694e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87d9e3b9",
   "metadata": {},
   "source": [
    "To see the meaning of each column, you can inspect the feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119cc79b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbc10d13",
   "metadata": {},
   "source": [
    "### Text features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a53dee",
   "metadata": {},
   "source": [
    "Another common need in feature engineering is to convert text to a set of representative numerical values. For example, most automatic mining of social media data relies on some form of encoding the text as numbers. One of the simplest methods of encoding data is by word counts: you take each snippet of text, count the occurrences of each word within it, and put the results in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da3798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = ['problem of evil',\n",
    "          'evil queen',\n",
    "          'horizon problem']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9f557f",
   "metadata": {},
   "source": [
    "One way to represent this is as a vector that counts up how often a word appears in a phrase. This would be tedious to do by hand. Luckily scikit learn has your back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36f557f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84f0927b",
   "metadata": {},
   "source": [
    "This is a bit hard to digest - we can convert it to a Pandas dataframe and look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34293940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a294633e",
   "metadata": {},
   "source": [
    "There are some issues with this approach, however: the raw word counts lead to features which put too much weight on words that appear very frequently, and this can be sub-optimal in some classification algorithms. One approach to fix this is known as term frequency-inverse document frequency (TF–IDF) which weights the word counts by a measure of how often they appear in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ffb9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c848c72e",
   "metadata": {},
   "source": [
    "### Derived features\n",
    "\n",
    "Sometimes it is possible to perform mathematical operations on the features or combinations of the features that mean they work much better in a given model. To get a feel for how this works consider the following data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f7d17a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af059c2b",
   "metadata": {},
   "source": [
    "We could try to fit this with a simple linear model, but it is clear that if we just use the values of the x axis in a linear model we will not get a very good result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cafc791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "482bf7ac",
   "metadata": {},
   "source": [
    "It's clear that we need a more sophisticated model to describe the relationship between x and y.\n",
    "\n",
    "One approach to this is to transform the data, adding extra columns of features to drive more flexibility in the model. For example, we can add polynomial features to the data this way.\n",
    "\n",
    "The derived feature matrix has one column representing $x$, and a second column representing $x^2$, and a third column representing $x^3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8b35e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367fa4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "611bed9e",
   "metadata": {},
   "source": [
    "### Missing data\n",
    "\n",
    "Another common need in feature engineering is handling of missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3edf91a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00b0e40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d34dec9c",
   "metadata": {},
   "source": [
    "We see that in the resulting data, the two missing values have been replaced with the mean of the remaining values in the column. This imputed data can then be fed directly into, for example, a `LinearRegression` estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdd31a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e195f11",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "\n",
    "We can see above that there is a workflow for processing and trating data, followed by passing it into models. We can obviously do this all with a sequence of code. But it might be much cleaner if we could group the steps together in a single function that executes the workflow. This would reduce the chance of missing/changing some steps when re-running the models. We can use the `Pipeline` class from scikit for this.\n",
    "\n",
    "For example, we might want a processing pipeline that looks something like this:\n",
    "\n",
    "    Impute missing values using the mean\n",
    "    Transform features to quadratic\n",
    "    Fit a linear regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b231877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec9029d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3edc249",
   "metadata": {},
   "source": [
    "# Advanced - Build a validation pipeline\n",
    "\n",
    "Let us set up a pipeline that can transform the data into different degrees of polynomial and then perform linear regression to obtain the coeffieients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551a2cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc298480",
   "metadata": {},
   "source": [
    "We can set up some test data and apply the fitted polynmial to that data, then plot the resuls.\n",
    "Which degree do we think best fits to the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7c9ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf1e04d0",
   "metadata": {},
   "source": [
    "We can use the scikitlearn validation curve function to investigate how the training and validation loss evolves with the complexity of the model. Here we look at polynomial degress from 0 to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e3a2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef88f54a",
   "metadata": {},
   "source": [
    "It looks like, beyond a third order polynomial the validation performance starts to deteriorate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd6aeb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d3f1fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc43158",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
