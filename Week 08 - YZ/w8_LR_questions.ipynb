{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43392cda",
   "metadata": {},
   "source": [
    "# 1. Classification with Naive Bayes\n",
    "\n",
    "We will initially do a classification with Naive Bayes. We will use this later on. But this code is not needed for the course work, so the code is provided here for you. If you finish early feel free to come back and look at this section.\n",
    "\n",
    "**Note** you still need to run the cells with the code, to make the next part work well.\n",
    "\n",
    "\n",
    "\n",
    "In the next part of the exercise - `Linear Regression` the code is not provided; that section will be important to know for your course work.\n",
    "\n",
    "* Load up the data that you cleaned up last week `week1-cleaned-data.pickle`\n",
    "* From this data build a vector X containing the independent variables (all columns except labels) and a vector y containing the labels\n",
    "* On this data perform a PCA so that the data is reduced to 2D\n",
    "* Train a Naive Bayes classifier on the resulting 2D data and the labels y\n",
    "* Calculate the binary cross entropy between the predicted values of the training set and the true values on the training set. **Hint** binary cross entropy is called log_loss in scikit-learn\n",
    "\n",
    "## 1.1 Load up the test test \n",
    "\n",
    "* Load up the test data from `test-data-week-2.pickle`\n",
    "* As above build  a vector `x_test` that has all the columns from this except labels and `y_test` that contains the labels\n",
    "* Transform this data using *the same* PCA as above *note* do **not** refit the PCA on this data\n",
    "* Apply the Naive Bayes classifier that we just trained to the test data and see how well it clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1235c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221ce02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('../Week 10/week1-cleaned-data.pickle')\n",
    "X = df_train.values[:, :9]\n",
    "y = df_train.values[:, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78767ad1",
   "metadata": {},
   "source": [
    "### Transform with a PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c7d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_out = pca.fit_transform(X)\n",
    "plt.scatter(pca_out[:, 0], pca_out[:, 1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f4da10",
   "metadata": {},
   "source": [
    "### Naive Bayes classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76acc866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(pca_out, y)\n",
    "\n",
    "ynew = model.predict(pca_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a898ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pca_out[:, 0], pca_out[:, 1], c=ynew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f904a171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "log_loss(ynew, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0684b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_pickle('test-data-week-2.pickle')\n",
    "x_test = df_test.values[:, :9]\n",
    "y_test = df_test.values[:, 9]\n",
    "x_test_pca = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31957300",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_test_pca[:, 0], x_test_pca[:, 1], c=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5ac56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ynew_test = model.predict(x_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31de7559",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_test_pca[:, 0], x_test_pca[:, 1], c=ynew_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5138e802",
   "metadata": {},
   "source": [
    "# 2. Linear regression\n",
    "\n",
    "This part of the exercise will be important for the course work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6291ebc4",
   "metadata": {},
   "source": [
    "### Set up the data\n",
    "\n",
    "* Using your classifier separate out all of the data that are ionic materials (label 0) from the training set\n",
    "\n",
    "```\n",
    "    ionic_indices = np.where(ynew==0)\n",
    "    df = df_train.iloc[ionic_indices]\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb4a720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10388bd3",
   "metadata": {},
   "source": [
    "### Drop unnecessary columns\n",
    "\n",
    "* Use pandas to drop the `labels` column from this dataset - [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html)\n",
    "\n",
    "```\n",
    "df.drop('labels', axis=1, inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b370949a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80320ef4",
   "metadata": {},
   "source": [
    "### Look at correlations\n",
    "\n",
    "We examine the data to see if any features are highly correlated.\n",
    "\n",
    "Look at a map of the Pearson correlations (see code from last week's exercise). On the basis of this **remove two of the columns** that are highly correlated to other columns. \n",
    "\n",
    "**NB** do not remove `band_gap` that is the column that you want to fit the model for.\n",
    "\n",
    "**NB** generally we will not remove columns unless correlation is > 0.95, but in this case you can if necessary.\n",
    "\n",
    "The code for the correlation matrix is in lecture 1 [of the ebook](https://keeeto.github.io/ebook-data-analysis/lecture-1-exploratory-data-analysis.html) - dont forget to `import seaborn as sns`\n",
    "\n",
    "Recall that the code to remove columns is:\n",
    "`df_reducded = df.drop([<list of columns to drop>], axis=1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13445500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d79cb362",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89671b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "844d312b",
   "metadata": {},
   "source": [
    "### Set up x and y and save the data\n",
    "\n",
    "x, the features for your model, are the first 6 columns (0-5)\n",
    "y, the target of your mode is the final column (6)\n",
    "\n",
    "The code to set this up and save the data is\n",
    "```\n",
    "x = df_reducded.values[:, :6]\n",
    "y = df_reducded.values[:, 6]\n",
    "df_reducded.to_pickle('week2-regression-train.pickle')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a310aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e7a8732",
   "metadata": {},
   "source": [
    "### Scale the data \n",
    "\n",
    "We then standardise the data using the `StandardScaler`. **Note** if the data has only one feature, like the label data y, we need to use a reshape when standardising. The code to do this is:\n",
    "```\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler_x = StandardScaler()\n",
    "x = scaler_x.fit_transform(x)\n",
    "scaler_y = StandardScaler()\n",
    "y = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cc404c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21b78595",
   "metadata": {},
   "source": [
    "### Split into train and test sets\n",
    "\n",
    "Use the train_test_split tool from `scikit-learn` to make an 80:20 training:test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154cac8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d846d209",
   "metadata": {},
   "source": [
    "### Set up a linear regression and fit the model\n",
    "\n",
    "Use `LinearRegression` from `scikit-learn` to fit the data.\n",
    "\n",
    "Look at the parameters of the final model:\n",
    "\n",
    "**Question** Which of the features seems to have the greatest influence on the band gap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354dd415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f7f3222",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0553af9b",
   "metadata": {},
   "source": [
    "### Analyse the performance of the model\n",
    "\n",
    "First we look at how well it predicts the training set.\n",
    "Use `predictions = regr.predict(x_train)` to make predictions.\n",
    "\n",
    "Use a scatter plot from matplotlib to plot `predictions` versus `y_train`. Don't forget to label your axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acfd09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a06ec295",
   "metadata": {},
   "source": [
    "Next do the same plot for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6fc33c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5ecfe20",
   "metadata": {},
   "source": [
    "### Summary statistics\n",
    "\n",
    "Use metrics from `scikit-learn` to look at the performace of the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d1b89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
